\section{Background and Motivation}

CrashSimulator operates in a unique niche in relation to other testing tools and techniques. As a result, it is better
suited to identify and report on certain types of errors that relate to the environment in which an application is
running.

    \subsection{What is an ``Environment''}

    An application's environment is the collection of all resources external to the application with which the
    application communicates.  These external resources can be thought of as an implicit input to the program as their
    presence and contents alter the flow of any application executed in their presence.  This work is concerned with
    bugs that arise from subtle differences in behavior between executions in one environment versus another
    specifically, the files and network communications visible through the monitoring the system calls the application
    makes during the course of its execution.  Consider the following hypothetical example: an application utilizing the
    POSIX API is executed on a Linux machine and an OS X machine.  Because the API's called in each situation
    superficially act the same, a developer might make the assumption that they are identical in all regards -- an
    assumption that will result in bugs in cases where it does not hold.  In the OS X and Linux example, an instance of
    this might be the differing contents returned in the output structure from a call to stat(), an API that
    superficially appears to be the same on both platforms.

    Another interesting case is the situation where the actual API's in question work correctly but the resources they
    operate on do not.  Consider the case where an application is executed on two different hosts with the same
    environment present locally but connected two different networks.  In this case, differences in the network itself
    can cause issues with the application's execution.  For example, if the application assumes that it will receive
    messages of a particular length from a network host it will not execute correctly when messages of that length are
    dropped, or fragmented, by a intermediary host with strict MTU requirements.

    \subsection{A Real World Example}

    Moving files around in a filesystem is an extremely common operation that, on face value, seems to fairly simplistic
    to carry out.  In reality, things become complex quickly once a few implementation details are taken into account:
    namely limitations in Linux's rename() system call and the way Linux's directory heirarchy can abstract away
    complicated device configurations -- a feature is usually a boon for developers.

    In this example, consider a Linux directory structure with two folders, /mnt/a/ and /mnt/b/, with each being a mount
    point for a different storage device.  Moving a file around anywhere inside either of these directories can be
    trivially accomplished by an application using the rename() system call.  Because it is common for systems to have
    the majority of their directory structures residing on the same device this approach appropriate.  Another case
    would be moving a file from /mnt/a/ to /mnt/b/.  In this case, rename() is incapable of performing the operation.
    When situation presents itself, an application must both detect the error returned by rename() and manually copy the
    file from one device to the other -- a complex process that is easy to carry out incorrectly.  \emph{Any}
    application that moves files around on a Linux system must be able to deal with this situation any part of the
    drirectory structure may physically reside on a different device.  For example, the standard log storage location,
    /var/log/, may be stored on a separate, higher speed, device in order to accomodate a high volume of reads and writes.

    During the course of its evaluation, we provided CrashSimulator with the computational models necessary to detect
    whether or not the applications being tested successfully performed all of the steps necessary to manually copy a
    file from one device to another.  We used the coreutils ``mv'' command source code as a guide for what constituted
    correct behavior as this utility is a key component of Linux systems and must be robust enough to correctly deal
    with any edge cases or unusual situations that arise.

    A variety of applications that move files around in a filesystem were tested using CrashSimulator with these
    capabilities enabled.  A particularly illustrative example is the ``shutils'' module that ships with Python 2.7.9.
    CrashSimulator was able to diagnose the module as failing on several points: it did not correctly preserve
    timestamps across the move, it did not correctly apply extended file attributes to the destination file after the
    move, and it did not verify the file's inode number between initially examining the source file and when the copy
    was initiated (a common cause of race conditions.)  After these tests were performed, we found an unresolved bug
    report from several years ago discussing this issue on Python's bug tracker -- a nice bit of confirmation of
    CrashSimulator's efficacy.

    This is just one example.  Given the variety of device types, file systems, and configurations possible it is not
    hard to imagine a wide extent of problematic combinations that could arise.  This is the sort of situation in which
    CrashSimulator excells.  CrashSimulator can induce error conditions or passively monitor an execution.  It can
    either look for the presence of good behavior (e.g. ``Did this application correctly copy extended file attributes
    when moving a file across disks?'') or it can identify the presence of bad behavior (e.g. ``Did this application try to
    interact with a block device as if it were a regular file?'').  
    
    \subsection{Existing Techniques}

    Existing tools can be roughly divided into two categories, black-box and white-box, based on the techniques they
    use to perform their testing. Black-box tools simply manipulate the inputs of the application under test and
    observe the resultant outputs. White-box tools, on the other hand, perform complex analysis of the application's
    source code in order to reason about what inputs are likely to produce interesting outputs. Each of these
    methodologies have their own advantages and disadvantages.

    White-box testing tools typically rely on a similar set of techniques, including constraint solving of branch
    statements in an application's code and symbolic execution of an application's code in order to generate inputs
    that optimally exercise the application's code paths. These techniques, while powerful, are not without their
    downsides. First, both techniques are computationally-expensive. Furthermore, symbolic execution can not always
    accurately represent actual execution and so there may be deviations in results. Similarly, efficiently solving
    a series of constraints in order to exercise a particular code path can be can be difficult to guarantee that a
    particular set of generated inputs will exercise the intended code path in many circumstances due to external
    dependencies that the tool cannot analyze. For example, a white-box testing tool cannot reliably generate inputs
    that are guaranteed to exercise a code path that relies on an operating system resource being available.
    Finally, white-box tools typically require that an application's source code be available which is not always
    the case. Even advanced white-box tools that analyze an application's machine code can be stymied in situations
    where an application's executable has been packed or encrypted.

    The alternative, black-box tools, have their own set of issues. They do not have an understanding of what an
    application is actually doing during execution which means they are only able to submit inputs and observe
    outputs.  The upside of this technique is simplicity. Black-box tools do not need the capability to understand
    and analyze an application's code which reduces their complexity immensely. Also, their testing process,
    mutating inputs and observing outputs, is computationally inexpensive. The downside of simplicity is that they
    cannot craft inputs with any sort of intelligence. This means that a great deal of time can be spent mutating
    inputs without much success in terms of bug identification. Also, they cannot identify specifically the source
    of faults in an application. They can only signal that a fault has occurred at some point during a test run.
    Furthermore, like white-box tools, these tools fail to take into account the environment in which the
    application is running.