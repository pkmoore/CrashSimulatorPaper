% TODO: Incorporate changes from Lois' markup
% TODO: Background And Motivation: Come up with some concrete examples
%    Linux app that runs fine on one network but not another
%    Write this as section II.d
%    Some sort of diagram
%    Talk at a high level -> likely to change depending on the evaluation


\section{Background and Motivation}

    This section describes the techniques and terminologies used in other automatic testing frameworks.

    \subsection{Existing Techniques}

        Existing tools can be roughly divided into two categories, black-box and white-box, based on the techniques they
        use to perform their testing. Black-box tools simply manipulate the inputs of the application under test and
        observe the resultant outputs. White-box tools, on the other hand, perform complex analysis of the application
        under test's source code in order to reason about what inputs are likely to produce interesting outputs. Each of
        these methodologies have their own advantages and disadvantages.

        White-box testing tools typically rely on a similar set of techniques including constraint solving of branch
        statements in an application's code and symbolic execution of an application's code in order to generate inputs
        that optimally exercise the application's code paths. These techniques, while powerful, are not without their
        downsides.  First, symbolic execution is computationally expensive and can result in deviations where the
        symbolic execution of code doesn't accurately represent the actual execution of code.  Similarly, efficiently
        solving a series of constraints in order to exercise a particular code path can be computationally expensive and
        it can be difficult to guarantee that a particular set of generated inputs will exercise the intended code path
        in many circumstances due to external dependencies that the tool cannot analyze. For example, a white-box
        testing tool cannot generate inputs that are guaranteed to exercise a code path that relies on an operating
        system resource being available. Finally, white-box tools typically require than an application's source code be
        available which is not always the case. Even advanced white-box tools that analyze an application's machine code
        can be stymied in situations where an application's executable has been packed or encrypted.

        The alternative, black-box tools, have their own set of issues. They do not have an understanding of what an
        application is actually doing during execution which means they are only able to submit inputs and observe
        outputs.  The upside of this technique is simplicity. Black-box tools do not need the capability to understand
        and analyze an application's code which reduces their complexity immensely. Also, their testing process,
        mutating inputs and observing outputs, is computationally inexpensive. The downside of simplicity is that they
        cannot craft inputs with any sort of intelligence. This means that a great deal of time can be spent mutating
        inputs without much success in terms of bug identification. Also, they cannot identify specifically the source
        of faults in an application. They can only signal that a fault has occurred at some point during a test run.
        Furthermore, like white-box tools, these tools fail to take into account the environment in which the
        application is running.

    \subsection{System Call Trace Analysis}

        CrashSimulator is based on prior work from two projects: NetCheck and CheckAPI. It, like these tools, analyzes
        system call traces captured from an execution of the application under test in order to determine locations that
        are likely to to be sources of faults. These traces are captured using strace and dtrace, the standard tools
        made available for this purpose in Linux and OS X respectively. Each system call is evaluated within the context
        of prior system calls, their parameters, and return values. This system call context is compared an ideal model
        and deviations from this model are logged.

    \subsection{Crash Simulation}

        Once CrashSimulator has generated a series of potential faults from its analysis of an application's system call
        trace the process of verifying the presence of faults can begin. Verification is handled by running an instance
        of the application and injecting one of the potential faults identified during the analysis step.  The results
        of this faulty execution are monitored and any deviations in output from the ``correct'' output are recorded.
        Faults that result in an application crashing are also recorded.
