\section{Evaluation}

    This work hopes to answer the following questions about CrashSimulator's operation:

        \begin{enumerate}
            \item{Is CrashSimulator successful in identifying flaws in new and existing applications?}
            \item{Is CrashSimulator able to generate tests in a performant manner?}
            \item{Is CrashSimulator able to execute tests in a performant manner?}
        \end{enumerate}

    In order to measure its efficacy CrashSimulator was implemented in Python. Python was chosen in order to facilitate
    code reuse from the previous work on CheckAPI and NetCheck. This implementation of CrashSimulator was evaluated on
    the basis of execution performance and number of bugs detected across a set of small test programs and larger, more
    mainstream applications.

    % TODO: Talk about the test platform, OS, Hardware, etc.
    \subsection{Execution Performance}

        One key attribute of successful testing tools is that they be able to complete their tests in a timely manner.
        If a tool takes too long to complete its tests users will be less likely to run it frequently, or at all,
        reducing the tools overall usefulness dramatically. To this end, the performance of CrashSimulator was evaluated
        in order to determine whether or not it was able to complete its test executions in an acceptable time frame.
        \textbf{There probably needs to be a citation here related to how long a developer is typically willing to wait
        for a test execution to complete}

        \subsubsection{Evaluation Against Sample Programs}

        Each of the following evaluations eamines the completion times for 20 consecurive executions of the specified
        application in both native and replay execution modes.  Multiple conseuritve executions were used for two
        reasons.  First, a single execution in either mode completes so quickly for the applications in question as to
        be nearly indistinguishable from the other mode.  Second, multiple executions somewhat contros for varying
        conditions like filesystem activity or network latency that can vary between individual executions.

            \begin{table}[H]
                \scriptsize{}
                \begin{tabular}{l  l  l  l}
                    \toprule{}
                        Execution Description & Native Eecution & Replay Execution\\
                        network\_speedtest & 0.024 & 2.407 \\
                        filesystem\_speedtest & 0.038 & 2.604 \\
                    \bottomrule{}
                \end{tabular}
            \end{table}

        \paragraph{Discussion on network\_speedtest}

        Sample application opens a TCP connection toan already running listener (netcat), sends a message, and
        exits. For simple applications like this, native execution is significantly faster than replay execution...

        \paragraph{Discussion on filesystem\_speedtest}

        This ample application create a new file with open(), writes a message to it, closes the file descriptor, and
        exit()'s. This test is particularly meaningful as it replicates a pattern of system calls that is used for in
        all sorts of applications in Linux...
            
            
        \subsubsection{Evaluation Against Major Programs}





    \subsection{Bugs Identified in Major Applications}


            \begin{table}[H]
                \scriptsize{}
                \begin{tabular}{l  l  l  l}
                    \toprule{}
                        Bug Tracker ID & Likely to be Identified & Identified & Notes \\
                        0001 & True & True & Note \\
                        0002 & True & True & Note \\
                        0003 & True & True & Note \\
                        0004 & True & True & Note \\
                        0005 & True & True & Note \\
                        0006 & False & False & Note \\
                        0007 & False & False & Note \\
                        0008 & False & False & Note \\
                        0009 & False & False & Note \\
                        0010 & False & False & Note \\
                    \bottomrule{}
                \end{tabular}
            \end{table}


    \subsection{Test Coverage Improvement}

        \textbf{\emph{There needs to be more exploration put into this test methodology. Much more description needs to
        be written.}}

    \subsection{Limitations}

        Evaluation of CrashSimulator has yielded the following limitations to be addressed by future work.

        \paragraph{Coupling to Architecture}

            As some faults injected by CrashSimulator require low level access to the test system's hardware or
            operating system data structures there exists some degree of coupling between CrashSimulator and these
            components. One area of expansion for CrashSimulator is support for more processor architectures and more
            operating systems.  CrashSimulator's test launcher as been designed in such a way that these improvements
            should be trivial to plug in once they have been implemented.

        \paragraph{Parallel Execution of Tests}

            In its current implementation, CrashSimulator injects faults in a serial manner. The primary cause of this
            design decision was the potential existence of dependencies between the application under test and finite
            operating system resources such as network ports, database connections, or hardware devices. Should an
            application under test have such a dependency it is non-trivial to transparently allocate this resource to
            the hundreds of instances of the application CrashSimulator could potentially launch simultaneously if it
            ran tests in parallel.

        % TODO: Verify this stuff
        \paragraph{Analysis of System Call Traces for Multi-Threaded Applications}

            Part of CrashSimulator's system call analysis is determining a global ordering for system calls based on the
            input traces. This is accomplished by making the assumption that all system calls are atomic and executed
            one at a time. This assumption does not hold in the case of multi-threaded applications. As a result, in
            these cases CrashSimulator is only able to inject faults that depend on single system calls.

        \paragraph{Testing of Applications With No Definite End Point}

            Some applications, typically daemons of some sort, are written in such a way that they will not end unless
            they are manually stopped if they encounter some fatal error condition. In these cases CrashSimulator must
            take a heuristic approach to its application monitoring post-injection. Cases where the application exhibits
            some invalid behavior immediately after injection are trivially identified. Cases where the injected fault
            causes a invalid behavior to become apparent at some undetermined future time are much more difficult to
            nail down. In these cases, CrashSimulator uses a user defined time period to kill the process and move on to
            the next test.

        \paragraph{Testing of Applications Written in Interpreted Languages}

            Because CrashSimulator operates on the system calls made by an application it does not make any attempt to
            determine what the cause of a fault may have been at a higher level. For example, in situations where
            CrashSimulator is testing an application written in an interpreted language the possibility exists that
            faults will be found in the interpreter rather than the application itself. For example, CrashSimulator may
            modify system calls made by the interpreter for purposes that are independent from the application under
            test.  If this results results in improper output CrashSimulator will simply report it as a fault in the
            application despite the fact that the user's code was not responsible for the error.

        \paragraph{Replay of System Call Traces for Complex Applications}


