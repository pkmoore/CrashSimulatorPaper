% TODO: Flesh this out a __LOT__ MORE
\section{Related Work}

    \subsection{Existing Techniques}

    Existing tools can be roughly divided into two categories, black-box and white-box, based on the techniques they
    use to perform their testing. Black-box tools simply manipulate the inputs of the application under test and
    observe the resultant outputs. White-box tools, on the other hand, perform complex analysis of the application's
    source code in order to reason about what inputs are likely to produce interesting outputs. Each of these
    methodologies have their own advantages and disadvantages.

    White-box testing tools typically rely on a similar set of techniques, including constraint solving of branch
    statements in an application's code and symbolic execution of an application's code in order to generate inputs
    that optimally exercise the application's code paths. These techniques, while powerful, are not without their
    downsides. First, both techniques are computationally-expensive. Furthermore, symbolic execution can not always
    accurately represent actual execution and so there may be deviations in results. Similarly, efficiently solving
    a series of constraints in order to exercise a particular code path can be can be difficult to guarantee that a
    particular set of generated inputs will exercise the intended code path in many circumstances due to external
    dependencies that the tool cannot analyze. For example, a white-box testing tool cannot reliably generate inputs
    that are guaranteed to exercise a code path that relies on an operating system resource being available.
    Finally, white-box tools typically require that an application's source code be available which is not always
    the case. Even advanced white-box tools that analyze an application's machine code can be stymied in situations
    where an application's executable has been packed or encrypted.

    The alternative, black-box tools, have their own set of issues. They do not have an understanding of what an
    application is actually doing during execution which means they are only able to submit inputs and observe
    outputs.  The upside of this technique is simplicity. Black-box tools do not need the capability to understand
    and analyze an application's code which reduces their complexity immensely. Also, their testing process,
    mutating inputs and observing outputs, is computationally inexpensive. The downside of simplicity is that they
    cannot craft inputs with any sort of intelligence. This means that a great deal of time can be spent mutating
    inputs without much success in terms of bug identification. Also, they cannot identify specifically the source
    of faults in an application. They can only signal that a fault has occurred at some point during a test run.
    Furthermore, like white-box tools, these tools fail to take into account the environment in which the
    application is running.
    \subsection{White-box Tools}

        White-box testing has been an area of intense interest in recent writing. Microsoft's SAGE and Bell Labs' DART
        are two examples of such tools that take different approaches to the same overall white-box technique.

        \subsubsection{DART}

            DART is a white-box testing tool that supports testing of applications written in the C programming language. It
            is capable of generating a test harness for an application's functions by through static analysis of the
            application's source code. This harness is then used for two phases of testing. First, it performs random
            testing and observes the application's behavior. Based on this random testing and symbolic execution of the
            application's source code, DART generates a series of inputs that will be used in the second phase of testing.
            These inputs are designed to direct the application down specific execution paths, observing the programs
            behavior and reporting faults as they are identified. DART operates on the assumption that the functions being
            evaluated have no side-effects and that the application is able to interact appropriately with its environment.
            More information can be found in \textbf{\emph{PAPER TITLE HERE}}

        \subsubsection{SAGE}

            SAGE differs from many other white-box testing tools in that it analyzes a compiled application's machine code
            rather than the application's uncompiled source code. This allows SAGE to operate on applications that were
            compiled from a variety of programming languages. It first runs the application under test with a set of well
            formed inputs and records an instruction-level trace of the application's execution. Next, it analyzes this
            trace in order to identify constraints that guard different paths of execution. SAGE then solves these
            constraints and, based on these solutions, generates inputs that are able to exercise specific paths of
            execution.

    \subsection{Black-box tools}

        % TODO: Find examples of black box tools

    \subsection{Trace Analysis Tools}

        Much of CrashSimulator's work on system call trace analysis is based on previous work on NetCheck and CheckAPI

        \subsubsection{NetCheck}

            This implementation of CrashSimulator relies of NetCheck for system call trace analysis. NetCheck uses two
            strategies to identify potential fault areas from system call trace. The first is a model based simulation
            of the system calls relevant to network communication from the input trace. System calls are organized
            according to a POSIX socket API dependency graph and prioritized based on the order in which the system
            calls should be made in an ideal scenario.  For example, a client application should not be making a
            \emph{connect} system call before it has set up its socket with the appropriate \emph{socket} system call.
            The model assumes that all system calls are atomic and that they cannot happen simultaneously. This allows a
            definite global order to be created.

            Once a global ordering is in place each system call is evaluated based on the previous system calls. Return
            values and parameters passed in are taken into account. If the system call is feasible it is accepted and
            the next system call is evaluated. If the system call is not feasible given the current system call context
            it is rejected and logged. In addition, system calls that return a value indicating some sort of network
            failure are recorded. After all system calls have been evaluated a NetCheck attempts to diagnose the
            source of any errors encountered. It is this diagnosis that CrashSimulator uses when deciding where and how
            to mutate the ``ideal run'' system call trace it is operating on.

        \subsubsection{CheckAPI}

            % TODO: Expand this
            \textbf{\emph{This needs to be expanded}}
            CheckAPI attempts to identify
