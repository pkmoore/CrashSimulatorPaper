% TODO: Introduction: Explicitly state the research questions we want to answer in the introduction
% TODO: Put a bulleted list of contributions into the introduction
\section{Introduction}

    One major hurdle in developing robust applications is anticipating and dealing with edge conditions and faults that
    compromise and applications ability to run as intended. A common source of faults is the environment in which an
    application runs. In an increasingly diverse software ecosystem it is not uncommon for breaking differences to
    appear between two different platforms. Whether it happens because of a lack of adherence to standards, intentional
    divergence, or developer error the result is the same: software written to run on multiple platforms falls victim to
    the differences between those platforms. These can easily go unnoticed due to the infeasibility of testing software
    across every combination of hardware and software on which it is expected to run. The typical approach to
    eliminating faults of any kind in an application is the creation of a suite of tests that exercise the applications
    functionality and ensure that it always acts appropriately. In the case of issues that arise from an application's
    environment it is not possible to enumerate and test for an acceptable amount of faults due to the variety of
    hardware and software systems that can make up the environment.

    This problem is of particular concern in the context of today's highly network dependent applications. With the
    explosion of mobile computing and software as a service products applications that are completely reliant on
    well-behaved network communication are increasingly common. As a result a great deal of emphasis must be placed on
    properly dealing with faults in these network communications. Unfortunately, identifying these faults is difficult
    due to the inability of the developer to replicate all possible application environments and exhaust all possible
    code paths.

    Existing tools can be roughly divided into two categories, black-box and white-box, based on the techniques they use
    to perform their testing. Black-box tools simply manipulate the inputs of the application under test and observe the
    resultant outputs. White-box tools, on the other hand, perform complex analysis of the application under test's
    source code in order to reason about what inputs are likely to produce interesting outputs. Each of these
    methodologies have their own advantages and disadvantages.

    White-box testing tools typically rely on a similar set of techniques including constraint solving of branch
    statements in an application's code and symbolic execution of an application's code in order to generate inputs that
    optimally exercise the application's code paths. These techniques, while powerful, are not without their downsides.
    First, symbolic execution is computationally expensive and can result in deviations where the symbolic execution of
    code doesn't accurately represent the actual execution of code. Similarly, efficiently solving a series of
    constraints in order to exercise a particular code path can be computationally expensive and it can be difficult to
    guarantee that a particular set of generated inputs will exercise the intended code path in many circumstances due
    to external dependencies that the tool cannot analyze. For example, a white-box testing tool cannot generate inputs
    that are guaranteed to exercise a code path that relies on an operating system resource being available. Finally,
    white-box tools typically require than an application's source code be available which is not always the case. Even
    advanced white-box tools that analyze an application's machine code can be stymied in situations where an
    application's executable has been packed or encrypted.

    The alternative, black-box tools, have their own set of issues. They do not have an understanding of what an
    application is actually doing during execution which means they are only able to submit inputs and observe outputs.
    The upside of this technique is simplicity. Black-box tools do not need the capability to understand and analyze an
    application's code which reduces their complexity immensely. Also, their testing process, mutating inputs and
    observing outputs, is computationally inexpensive. The downside of simplicity is that they cannot craft inputs with
    any sort of intelligence. This means that a great deal of time can be spent mutating inputs without much success
    in terms of bug identification. Also, they cannot identify specifically the source of faults in an application. They
    can only signal that a fault has occurred at some point during a test run. Furthermore, like white-box tools, these
    tools fail to take into account the environment in which the application is running.

    In light of these shortcomings, an alternative approach is needed to provide developers with confidence that their
    applications are sufficiently free of environment induced faults.  CrashSimulator is a tool designed to meet this
    need by providing automated identification of environment-induced faults through simulated deviations from the
    normal flow of system calls an application makes when interacting with its environment. Like other white-box tools,
    CrashSimulator attempts to analyze an application in order to understand where it can inject faults.  However,
    unlike other tools it bypasses the application's source code entirely --- instead relying on system call traces.
    This reduces CrashSimulator's footprint by eliminating complex language analysis code while still allowing insight
    into the application's operation. At the same time, CrashSimulator, like other black-box tools, is able to test
    applications in the absence of source code. Furthermore, because it bases its analysis on a successful run of the
    application in a real environment this analysis takes into account the application's interactions with its
    environment. During its analysis, CrashSimulator identifies sets of system calls that represent areas likely to
    contain faults. Once analysis is complete, CrashSimulator runs the application under test and injects the faults it
    identified previously and reports to the developer whether or not the application successfully handled them or not.
    For example, CrashSimulator might identify a system call that rarely fails and, as a result, this failure is rarely
    handled in practice. It can then inject this failure and observe the results.
