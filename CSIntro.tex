\section{Introduction}

One major hurdle in developing robust applications is getting them to run reliably across all of the environments on
which they may be installed.  In an increasingly diverse software ecosystem it is not uncommon for problematic
differences to appear between these environments.  Whether it happens because of a lack of adherence to standards or
developer error, the result is the same: software written to run in multiple environments falls victim to the
differences between those environments.  These can easily go unnoticed due to the infeasibility of testing software
across every combination of hardware and software on which it is expected to run.  Manually testing an application in
all production environments is impossible in practice. Developers are often unaware of particular types of environment
anomalies, leaving their applications riddled with environment-dependent bugs that are only detected \emph{after
  deployment}.

Existing tool support for automated testing of software against environment anomalies is very limited, making it
extremely difficult for developers to tackle this problem systematically.  It is therefore not surprising that
environment-related bugs are common in real world software. A recent survey conducted by
ClusterHQ~\cite{ClusterHQSurvey} confirmed that app developers spend a significant portion of their time debugging
errors discovered in production.  The survey participants cited the inability to recreate production environments for
testing as the main reason why bugs are not discovered earlier in the development process.  What is needed is a way to
execute an application in such a way that it experiences the environmental anomalies present in a deployment environment
without having to actually deployment.  This work introduces an an approach to do just that.

Applications interact their environment by making requests in the form of system calls and receiving responses in the
form of the results and side effects generated by the these system calls.  From an application's perspective, these
interactions contain the small differences that make a particular environment unique.  For example, in Linux the {\tt
  select()} system call in some environments correctly updates the state of a variable tracking the length of time the
call blocked for while in other environments this variable is not correctly updated -- meaining applications that depend
on one behavior or another will fail when the call acts differently.  Based on this concept, it is possible to represent
an environment as a set of differences in system call interactions from some point of reference environment.  This means
a given target environment can be simulated, from the applications perspective, by influencing the results and side
effects of the system calls the applications makes such that they contain the differences representative of the target
environment.  An application can be tested as if it were running in the target environment by creating a simulation of
the environment using this technique and executing the application within it.

Given that we have a simulation of a given environment, the next step is to make decisions about whether an application
is executing correctly in this environment.  The set of interactions (i.e. system calls) necessary to correctly perform
a given operation in a given environment can be thought of as a protocol that the application must follow.  With this
idea in mind it is possible to monitor the execution of an application and, at each system call, update its state as it
carries out the protocol.  At the end of execution this this state can examined in order to determine whether or not the
appliation successfully completed the protocol and, as a result, performed the operation correctly in terms of the
simulated environment.

Individual pieces of the above process can be thought of as a pair consisting of an environmental anomaly and a model
that determines whether or not the application handled it correctly.  Each of these pairs is a test and collectively
they can be thought of as a test suite.  A key advantage of this approach is that these tests and test suites can be
accumulated from some number of applications that exist in the environment the tests will describe.  Once accumulated,
they can be used to test any application as if it were running in that environment.  In this way, prior knowledge
gathered from past deployment experiences can be used to more thoroughly exercise a new application so that it can be
constructed in such a way that it handles the challenges of the target environment correctly.

From a usability perspective, the above provides a repeatable way to inject anomalous behavior into executions of any
application that needs to be tested.  Essentially, a ``test suite'' of interesting injectable environment conditions can
be accumulated from work across some number projects and applied across another project that needs to be tested.  When
these ``tests'' are collected based on issues experienced in other environments, the collection of ``tests''
approximates the unusual conditions that any application might expect to encounter if it was run in this environment.
By executing any application against the ``suite'' of ``tests'' gathered from a particular environment, CrashSimulator
is effectively executing the application inside a simulation of that environment complete with all its peculiarity.  In
this way, CrashSimulator is able to give developers a sense of how an application will perform in that evironment so
that they can construct to deal with the enviroment correctly.

CrashSimulator achieves this goal by analyzing and manipulating the execution of the application being tested.
Specifically, CrashSimulator ``replays'' a previously recorded system call trace of the application being tested and
uses the control this gives in order to observe the system calls an application makes and to intercede at specific
points in order to direct execution down a path that is interesting from a testing perspective.

In order to evaluate the effectiveness of this approach this work introduces CrashSimulator.  CrashSimulator provides
environment simulation by replaying a system call trace of an application while intervening at appropriate times in
order to inject the anomalies that make the target environment unique.  During the course of each execution
CrashSimulator evaluates the applications behavior by providing the sequence of system calls it makes to deterministic
computational models that encode protocols required for the application to interact with the simulated environment
successfully.  Experiments using CrashSimulator have shown that it can rapidly perform a high volume of replay
executions in simulated environments.  CrashSimulator has also been successful in identifying bugs in several popular
applications.  Testing these applications by executing them in an environment provided by CrashSimulator while
monitoring the execution using models that encoded the protocol around moving files across storage devices in Linux
resulted in the discovery of new bugs with effects ranging from application crashes to loss of data.

This work makes the following contributions:

    \begin{enumerate}
        \item{A new approach that uses targeted manipulation of the interactions between an application and its
            environment to test how an appication will respond in a given environment without having to deploy
            the application to that environment.}
        \item{An approach for encoding an appropriate flow of interactions between an application and its environment as
            a model that can be later used to check for correct behavior}
        \item{A tool that implements the discussed approach that is able to successfully find bugs, both known and
            unknown, in popular applications.}
    \end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "CrashSimulator"
%%% End: 
