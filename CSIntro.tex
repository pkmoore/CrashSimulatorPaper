\section{Introduction}

One major hurdle in developing robust applications is getting them to
run reliably across all of the environments in which they may be
installed.  In an increasingly diverse software ecosystem it is not
surprising then that problematic differences have emerged between these
environments.  Whether due to a failure to adhere to 
standards or developer error, the result is the same: the software 
fails because of unanticipated environmental conditions.

The conditions referred to here, which can include API behavior
differences, variations in network performance, or peculiarities in
filesystem state, all
can easily go unnoticed
during system testing because it is not feasible to
test programs 
across every possible combination of hardware and software on which it
is expected to run. Moreover, developers are often unaware
of particular types of environmental anomalies, and their
applications contain environment-dependent bugs that are only
detected \emph{after deployment}. A recent survey conducted by
ClusterHQ~\cite{ClusterHQSurvey} confirmed that application developers
spend a significant portion of their time debugging errors that are only discovered
in production.  The survey participants cited the inability to
recreate production environments for testing as the main reason why
bugs are not discovered earlier.
Even specialized environments, such as the Java Runtime Environment, that
attempt to hide these environmental differences are not perfect.
In many cases, they end up increasing both the difficulty of debugging and
development costs.

What is needed is a strategy for executing an application in such a way that
it can experience the anomalies of a deployment
environment without actually having to run in one.

%
%For example, in some environments the {\tt select()} system call
%correctly updates the state of a variable tracking the length of time
%the call was blocked, while in other environments this variable is not
%correctly updated.  Consequently, applications that depend on one
%behavior or another may fail when the call acts
%differently. Similarly, consider an application that has been
%developed with the assumption that the socket returned by {\tt
%  accept()} will inherit the settings configured on the parent socket.
%This is how {\tt accept()} behaves in the canonical BSD sockets
%implementation so this is a reasonable assumption.  Unfortunately,
%this is not the case in Linux where the returned socket does not
%inherit these options causing subsequent system calls to misbehave if
%the application does not account for the anomaly. In fact, this
%anomaly resulted in a portability bug in
%Python~\cite{Zhuang_NSDI_2014}.  Interactions with file systems are
%also prone to small differences in environments.  In testing GnuPG we
%discovered that developers implicitly assumed that a file being
%provided as input to the key import process was a regular file; when
%this assumption was violated, the application failed.
%

In this paper, we introduce {\em CrashSimulator}, a testing approach
and tool that
realizes such a strategy. CrashSimulator is based on the
observation that: {\em Insight gleaned from a fault that is
  caused by unexpected interactions of {\em one application} with one
  of its deployment environments, can be leveraged to discover related
  faults in a {\em wide range} of applications that may run in that
  environment.}  The approach is loosely inspired by the 
environments provided by flight simulators, which  will use the knowledge that
one model
of aircraft is prone to behave poorly under certain environmental
conditions (e.g.\ windy conditions, rain, etc) to
test many other aircraft models in simulators against relevant
aspects of the problematic environment.  In the case of CrashSimulator,
we are concerned with the implicit inputs, implementation details, and resource
constraints an application's environment supplies.  Rather than stormy
weather and foggy landings, applications experience anomalous conditions such as varying
kernel versions with slightly different implementations details,
unusual filesystem configurations, and unexpected network conditions that
affect performance. Our approach tests an application against such anomalous
conditions by modifying the responses to the
application's system calls, thereby simulating potential deployment environments.


The development of the CrashSimulator tool for automated testing of
software against an environment is an important step forward.  There
are other testing techniques that follow a similar
approach~\cite{mariani2007compatibility,
  DBLP:journals/ase/WasylkowskiZ11, DBLP:conf/icse/PradelJAG12,
  DBLP:journals/tosem/MonperrusM13, DBLP:conf/icse/JamrozikSZ16},
including techniques that detect bugs involving individual system
calls~\cite{Koopman00theexception,Dadeau:2008:CSM:1433121.1433137,Farchi02}.
However, a correct response to an anomaly triggered by one system call
often requires the application to make several other system calls,
each of which may trigger further anomalies. Tools such as
NetCheck~\cite{Zhuang_NSDI_2014} and
CheckAPI~\cite{rasley2015detecting} can analyze complex response
patterns while the anomaly is observed in the deployment
environment. However, they do not support systematic testing of
applications before deployment.  As a result, relying only on these
capabilities results in applications being deployed with a significant
number of bugs that are only discovered after the fact ---
necessitating costly bug triage and application re-deployment.
We are the first to
explore techniques for finding bugs related to complex
environment interactions that involve multiple interdependent system
calls, before the application is deployed. 


% CrashSimulator is concerned with bugs that arise from subtle
% differences in behavior between executions in one environment versus
% another, such as the files and network communications visible to the
% application through the system calls the application makes during the
% course of its execution.

% In each of these cases, when problems of this nature are discovered in one application, not only should that application
% be fixed, but a huge group of other applications that use similar constructs and that may run in the problematic
% environment should also be tested to see whether they have the same problems. CrashSimulator facilitates that task.

%Since an application's interactions with its environment are mediated
%by system calls, it is possible to represent the difference between
%the environment in which testing is performed (referred to here as the
%{\em test environment}) and the environment under simulation (referred
%to here as the {\em anomalous environment}) as a set of differences in
%system call interaction results. This means a given target
%environment can be simulated, from the application's perspective, by
%influencing the results and side effects of the system calls the
%applications makes such that they appear to be coming from the target
%environment.  CrashSimulator interposes on system calls to allow an
%application to be tested as if it were running in the target
%environment by creating a simulation of the environment using this
%technique and executing the application within it.

Here is how CrashSimulator works in detail. Suppose an application
$A$ has failed due to a newly discovered environmental anomaly.  A
developer using CrashSimulator can then test whether other
applications $A_1$, $A_2,\dots$ suffer from the same problem.  The
first step is to run these other applications in their development
environment in order to gather traces.  Next, she writes a simple
script to evaluate whether traces are relevant to this anomaly and to
mutate traces to give the appearance that they were generated in the
anomalous environment.  Finally, she attempts to replay the modified
trace, i.e. allow the application to run under the control of
CrashSimulator, substituting the system call return values (and
side-effects) that would occur in the anomalous environment. In
parallel with the replay, a checker will run under the control of
CrashSimulator that decides whether the application's behavior is
indicative of a fault.

%Given that we have a simulation of a given environment, the next step is to make decisions about whether an application
%is executing correctly in this environment.  The set of interactions (i.e. system calls) necessary to correctly perform
%a given operation in a given environment can be thought of as a protocol that the application must follow.  With this
%idea in mind it is possible to monitor the execution of an application and, at each system call, update its state as it
%carries out the protocol.  At the end of execution this this state can examined in order to determine whether or not the
%appliation successfully completed the protocol and, as a result, performed the operation correctly in terms of the
%simulated environment.
%
%
%Individual pieces of the above process can be thought of as a pair consisting of an environmental anomaly and a model
%that determines whether or not the application handled it correctly.  Each of these pairs is a test and collectively
%they can be thought of as a test suite.  
A key advantage of this approach is that these %tests and test suites can be
trace perturbations and corresponding checkers can be
accumulated from any number of applications that exist in the environment the tests will describe.  Once accumulated,
they can be used to test any application as if it were running in that environment.  In this way, prior knowledge
gathered from past deployment experiences can be used to more thoroughly work
out a new application so that it can be
constructed in such a way that it handles the challenges of the target environment correctly.

%MOVE NEXT FEW PARAGRAPHS TO SUMMARY IN CONCLUSION ???
%
%From a usability perspective, the above provides a repeatable way to inject anomalous behavior into executions of any
%application that needs to be tested.  Essentially, a ``test suite'' of interesting injectable environment conditions can
%be accumulated from work across some number projects and applied across another project that needs to be tested.  When
%these ``tests'' are collected based on issues experienced in other environments, the collection of ``tests''
%approximates the unusual conditions that any application might expect to encounter if it was run in this environment.
%By executing any application against the ``suite'' of ``tests'' gathered from a particular environment, CrashSimulator
%is effectively executing the application inside a simulation of that environment complete with all its peculiarity.  In
%this way, CrashSimulator is able to give developers a sense of how an application will perform in that evironment so
%that they can construct to deal with the enviroment correctly.
%
%CrashSimulator achieves this goal by analyzing and manipulating the execution of the application being tested.
%Specifically, CrashSimulator ``replays'' a previously recorded system call trace of the application being tested and
%uses the control this gives in order to observe the system calls an application makes and to intercede at specific
%points in order to direct execution down a path that is interesting from a testing perspective.
%
%In order to evaluate the effectiveness of this approach this work introduces CrashSimulator.  CrashSimulator provides
%environment simulation by replaying a system call trace of an application while intervening at appropriate times in
%order to inject the anomalies that make the target environment unique.  During the course of each execution
%CrashSimulator evaluates the applications behavior by providing the sequence of system calls it makes to deterministic
%computational models that encode protocols required for the application to interact with the simulated environment
%successfully.  Experiments using CrashSimulator have shown that it can rapidly perform a high volume of replay
%executions in simulated environments.  CrashSimulator has also been successful in identifying bugs in several popular
%applications.  Testing these applications by executing them in an environment provided by CrashSimulator while
%monitoring the execution using models that encoded the protocol around moving files across storage devices in Linux
%resulted in the discovery of new bugs with effects ranging from application crashes to loss of data.
%
%END -- move to conclusion

%To illustrate CrashSimulator's operation, consider applications that
%at some point ``move'' a file from a source filename to a destination
%filename.  Here, the relevant aspect of the environment is the state
%of the filesystem.  In many environments, it is sufficient for the
%application to simply call the {\tt rename()} system call, so having
%tested the application only in such environments, developers might
%erroneously conclude that the application handles the ``move''
%correctly.  However, there are many situations (environments) in which
%this is not sufficient, including environments involving symbolic
%links to files, cross-device moves, and/or special files.  In this
%paper, we will illustrate CrashSimulator's operation on an example in
%which a special file is being moved to a different device.  This
%anomaly is handled incorrectly by several widely used
%applications. Other similar anomalies that CrashSimulator can detect
%are discussed in section~\ref{sec:evaluation}.
%
%After discovering that some application $A$ fails in an environment
%where it tries to move a special file (such as {\tt /dev/urandom)}
%across devices, crash simulator can test other applications $A'$ as
%follows:
%\begin{enumerate}
%\item Run the application under test, $T$, in its normal testing environment (e.g. regular file is
%  moved on the same device), recording a trace of $T$'s system calls,
%  including their arguments and return values.
%\item Check that this trace satisfies some of the requirements for
%  handling the anomalous environment correctly. In this case, the
%  trace must include a call to {\tt stat()} or a related system call
%  to check the status of the file being moved. If it does not, the
%  application almost certainly will not work correctly in the
%  anomalous environment.
%\item Execute the application under the control of CrashSimulator,
%  replaying the trace up to the point of the {\tt stat()} call. Here,
%  each time the application makes a system call, CrashSimulator
%  intervenes, supplying return values and causing side effects as the
%  actual call would, without executing the actual call.
%\item Modify the structure returned by the {\tt stat()} call so that
%  it returns values indicating that the file is a special file,
%  i.e. so it simulates execution in the anomalous environment.
%\item Continue execution of $T$ under the control of
%  CrashSimulator. Here the series of system calls made by the
%  application may diverge from the original trace, as $T$ responds to
%  the modified return value from the {\tt stat()} call. CrashSimulator
%  feeds the series of system calls made into a {\em checker},
%  customized for this particular anomaly, that determines whether the
%  application is attempting to respond to the anomaly and either
%  reports a bug (failure to respond to the anomaly) or success (the
%  application appears to be making an effort to respond to the
%  anomaly).
%\end{enumerate}

Each of CrashSimulator's test patterns consists of rules for
mutating traces to give the application the illusion that it is
executing in an anomalous environment, and a checker that attempts to
determine whether the application responds appropriately to the
anomaly.  Once developed, these test patterns can be applied to many
traces from many applications that may be prone to mishandling the
environmental anomaly in question.


This paper makes the following contributions:

\begin{enumerate}
\item{It proves the importance of the interaction between an application and
    its environment in creating potential flaws upon deployment.}
\item{It proposes the idea that by manipulating these interactions through
    the use of system calls, the responses of an application to any given
    environment can be accurately simulated without an actual deployment to
    that environment.}
\item{It offers an approach for encoding an appropriate flow of
    interactions between an application and its environment as a model that
    can be later used to check for correct behavior.}
\item{It presents an implementation of CrashSimulator that can implement the
    discussed approach to successfully find bugs, both known and unknown. 
    Using this prototype on popular applications, such as {\tt wc}, {\tt ftp}, 
    {\tt vim}, and {\tt nano}, we found 84 bugs.}
\end{enumerate}

In the remainder of this paper we
offer additional details on CrashSimulator's operation.
In section~\ref{sec:approach} we present the rationale for our system call trace
and replay system and explain how anomalies are identified, extracted,
converted, and injected while section~\ref{sec:evaluation} analyzes the results
of testing conducted with the tool and evaluates its effectiveness at checking
behavior in simulated environments.  Related work is discussed in
section~\ref{sec:relatedwork} and we offer concluding thoughts and suggestions
for future work in section~\ref{sec:conclusion}.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "CrashSimulator"
%%% End: 
